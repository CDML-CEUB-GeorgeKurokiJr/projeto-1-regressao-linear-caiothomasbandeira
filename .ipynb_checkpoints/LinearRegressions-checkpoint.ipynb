{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c2d43da",
   "metadata": {},
   "source": [
    "# üìä Regress√£o Linear Simples ‚Äî Precifica√ß√£o de Risco em Empr√©stimos\n",
    "## Risk-Based Pricing com Lending Club Loan Data\n",
    "\n",
    "**Autor:** Caio Thomas Silva Bandeira  \n",
    "**Disciplina:** Ci√™ncia de Dados para Engenheiros ‚Äî Deep Learning  \n",
    "**Institui√ß√£o:** CEUB ‚Äî Centro Universit√°rio de Bras√≠lia  \n",
    "**Professor:** George Kuroki Jr.\n",
    "\n",
    "---\n",
    "\n",
    "### Objetivo do Projeto\n",
    "\n",
    "Construir e avaliar **3 modelos de Regress√£o Linear Simples**, cada um usando uma vari√°vel independente diferente para prever a **taxa de juros (`int_rate`)** de empr√©stimos. Cada modelo √© implementado via:\n",
    "\n",
    "1. **statsmodels** ‚Äî Sum√°rio estat√≠stico completo (OLS)\n",
    "2. **TensorFlow/Keras** ‚Äî Rede neural simples (single-neuron)\n",
    "\n",
    "O foco √© o **rigor estat√≠stico** (verifica√ß√£o de premissas) e a **clareza did√°tica**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac854ed5",
   "metadata": {},
   "source": [
    "---\n",
    "## 1 ¬∑ Configura√ß√£o do Ambiente e Importa√ß√£o de Bibliotecas\n",
    "\n",
    "Importamos todas as bibliotecas necess√°rias para o projeto. Cada uma tem um papel espec√≠fico:\n",
    "\n",
    "| Biblioteca | Papel |\n",
    "|---|---|\n",
    "| `pandas` / `numpy` | Manipula√ß√£o e estrutura de dados |\n",
    "| `matplotlib` / `seaborn` | Visualiza√ß√µes e gr√°ficos estat√≠sticos |\n",
    "| `statsmodels` | Regress√£o OLS com sum√°rio estat√≠stico detalhado (p-values, R¬≤, F-stat) |\n",
    "| `tensorflow/keras` | Rede neural simples (1 neur√¥nio linear) |\n",
    "| `scipy` | Testes estat√≠sticos (Shapiro-Wilk) |\n",
    "| `kagglehub` | Download autom√°tico do dataset do Kaggle |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c44e63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. IMPORTA√á√ÉO DE BIBLIOTECAS\n",
    "# =============================================================================\n",
    "# Cada importa√ß√£o √© agrupada por fun√ß√£o para facilitar a leitura e manuten√ß√£o.\n",
    "\n",
    "# --- Supress√£o de warnings ---\n",
    "# Fazemos isso logo no in√≠cio para que o notebook n√£o fique polu√≠do com avisos\n",
    "# do TensorFlow e de deprecia√ß√µes que n√£o afetam nosso resultado.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suprime logs internos do TensorFlow\n",
    "\n",
    "# --- Manipula√ß√£o de Dados ---\n",
    "import pandas as pd   # Estrutura de DataFrames (tabelas)\n",
    "import numpy as np    # Opera√ß√µes num√©ricas vetorizadas\n",
    "\n",
    "# --- Visualiza√ß√£o ---\n",
    "import matplotlib.pyplot as plt  # Gr√°ficos est√°ticos (base)\n",
    "import seaborn as sns            # Gr√°ficos estat√≠sticos de alto n√≠vel\n",
    "\n",
    "# --- Modelagem Estat√≠stica (statsmodels) ---\n",
    "# O statsmodels nos d√° o \"sum√°rio completo\" da regress√£o: p-values, R¬≤,\n",
    "# intervalos de confian√ßa, F-statistic ‚Äî tudo que precisamos para rigor estat√≠stico.\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.gofplots import qqplot          # Q-Q Plot para normalidade dos res√≠duos\n",
    "from statsmodels.stats.stattools import durbin_watson      # Teste de independ√™ncia dos res√≠duos\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor  # VIF (multicolinearidade)\n",
    "\n",
    "# --- Deep Learning (TensorFlow / Keras) ---\n",
    "# Usaremos uma rede neural com UM √öNICO neur√¥nio linear para demonstrar que\n",
    "# √© algebricamente id√™ntica √† regress√£o linear: ≈∑ = Wx + b ‚â° ≈∑ = Œ≤‚ÇÅx + Œ≤‚ÇÄ\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# --- Testes Estat√≠sticos (SciPy) ---\n",
    "from scipy.stats import shapiro  # Teste de Shapiro-Wilk para normalidade dos res√≠duos\n",
    "\n",
    "# --- Download do Dataset ---\n",
    "import kagglehub  # Download autom√°tico de datasets do Kaggle\n",
    "\n",
    "# =============================================================================\n",
    "# FUN√á√ïES AUXILIARES ‚Äî substituem funcionalidades que eram do sklearn\n",
    "# =============================================================================\n",
    "\n",
    "def train_test_split_manual(X, y, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Divis√£o manual treino/teste usando numpy (substitui sklearn.model_selection.train_test_split).\n",
    "    \n",
    "    Par√¢metros:\n",
    "        X: array de features (n, p)\n",
    "        y: array de target (n,)\n",
    "        test_size: propor√ß√£o do teste (0.0 a 1.0)\n",
    "        random_state: seed para reprodutibilidade\n",
    "    \n",
    "    Retorna:\n",
    "        X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    n = len(y)\n",
    "    indices = rng.permutation(n)\n",
    "    split_idx = int(n * (1 - test_size))\n",
    "    train_idx, test_idx = indices[:split_idx], indices[split_idx:]\n",
    "    return X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "\n",
    "def r2_score_manual(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    R¬≤ (Coeficiente de Determina√ß√£o) ‚Äî propor√ß√£o da vari√¢ncia explicada.\n",
    "    R¬≤ = 1 - SS_res / SS_tot\n",
    "    \"\"\"\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    return 1 - (ss_res / ss_tot)\n",
    "\n",
    "def rmse_manual(y_true, y_pred):\n",
    "    \"\"\"RMSE ‚Äî Raiz do Erro Quadr√°tico M√©dio.\"\"\"\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "def mae_manual(y_true, y_pred):\n",
    "    \"\"\"MAE ‚Äî Erro M√©dio Absoluto.\"\"\"\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "def standard_scale(X_train, X_test=None):\n",
    "    \"\"\"\n",
    "    Padroniza√ß√£o z-score manual (substitui sklearn.preprocessing.StandardScaler).\n",
    "    z = (x - Œº) / œÉ\n",
    "    \n",
    "    Retorna:\n",
    "        X_train_scaled, X_test_scaled (se X_test fornecido), mean, std\n",
    "    \"\"\"\n",
    "    mean = np.mean(X_train, axis=0)\n",
    "    std = np.std(X_train, axis=0)\n",
    "    X_train_scaled = (X_train - mean) / std\n",
    "    if X_test is not None:\n",
    "        X_test_scaled = (X_test - mean) / std\n",
    "        return X_train_scaled, X_test_scaled, mean, std\n",
    "    return X_train_scaled, mean, std\n",
    "\n",
    "# --- Configura√ß√µes Globais de Visualiza√ß√£o ---\n",
    "sns.set_style('whitegrid')                           # Estilo limpo com grade\n",
    "plt.rcParams['figure.figsize'] = (12, 6)             # Tamanho padr√£o dos gr√°ficos\n",
    "plt.rcParams['font.size'] = 12                       # Tamanho de fonte leg√≠vel\n",
    "plt.rcParams['axes.titlesize'] = 14                  # T√≠tulos de eixo maiores\n",
    "\n",
    "# --- Reprodutibilidade ---\n",
    "# Fixamos a seed em TODOS os geradores de n√∫meros aleat√≥rios para garantir\n",
    "# que qualquer pessoa que rode este notebook obtenha exatamente os mesmos resultados.\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "\n",
    "print(\"‚úÖ Todas as bibliotecas foram importadas com sucesso!\")\n",
    "print(f\"   Pandas:       {pd.__version__}\")\n",
    "print(f\"   NumPy:        {np.__version__}\")\n",
    "print(f\"   Matplotlib:   {plt.matplotlib.__version__}\")\n",
    "print(f\"   Seaborn:      {sns.__version__}\")\n",
    "print(f\"   Statsmodels:  {sm.__version__}\")\n",
    "print(f\"   TensorFlow:   {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4324c2e",
   "metadata": {},
   "source": [
    "---\n",
    "## 2 ¬∑ Download e Carregamento do Dataset (Lending Club via KaggleHub)\n",
    "\n",
    "O **Lending Club Loan Data** √© um dos maiores datasets p√∫blicos de cr√©dito, contendo informa√ß√µes de ~2,2 milh√µes de empr√©stimos concedidos entre 2007 e 2018.\n",
    "\n",
    "Utilizamos a biblioteca `kagglehub` para download autom√°tico. Na primeira execu√ß√£o, ser√° necess√°rio autenticar com sua chave de API do Kaggle.\n",
    "\n",
    "> **Nota:** O dataset original possui mais de 150 colunas. Carregaremos todas inicialmente e depois selecionaremos apenas as relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c097ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2. DOWNLOAD E CARREGAMENTO DO DATASET\n",
    "# =============================================================================\n",
    "# O kagglehub baixa o dataset e retorna o caminho local onde foi salvo.\n",
    "# Na primeira execu√ß√£o, pode ser necess√°rio configurar a chave de API do Kaggle.\n",
    "# Instru√ß√µes: https://www.kaggle.com/docs/api\n",
    "\n",
    "print(\"‚è≥ Baixando o dataset do Kaggle (pode demorar na primeira vez)...\")\n",
    "path = kagglehub.dataset_download('wordsforthewise/lending-club')\n",
    "print(f\"‚úÖ Dataset baixado em: {path}\")\n",
    "\n",
    "# --- Localizar o arquivo CSV dentro da pasta baixada ---\n",
    "# O kagglehub pode baixar m√∫ltiplos arquivos; precisamos do principal.\n",
    "import glob\n",
    "csv_files = glob.glob(os.path.join(path, '**', '*.csv'), recursive=True)\n",
    "print(f\"\\nüìÇ Arquivos CSV encontrados:\")\n",
    "for f in csv_files:\n",
    "    print(f\"   ‚Üí {os.path.basename(f)}\")\n",
    "\n",
    "# --- Carregar o CSV principal ---\n",
    "# Usamos low_memory=False porque o arquivo √© grande e tem colunas com tipos mistos.\n",
    "# Isso evita warnings de infer√™ncia de tipo inconsistente entre os chunks de leitura.\n",
    "csv_path = [f for f in csv_files if 'accepted' in os.path.basename(f).lower() or 'loan' in os.path.basename(f).lower()]\n",
    "csv_path = csv_path[0] if csv_path else csv_files[0]\n",
    "\n",
    "print(f\"\\n‚è≥ Carregando: {os.path.basename(csv_path)} ...\")\n",
    "df_raw = pd.read_csv(csv_path, low_memory=False)\n",
    "\n",
    "print(f\"‚úÖ Dataset carregado com sucesso!\")\n",
    "print(f\"   Shape original: {df_raw.shape[0]:,} linhas √ó {df_raw.shape[1]} colunas\")\n",
    "\n",
    "# --- Visualizar as primeiras linhas ---\n",
    "df_raw.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baff3870",
   "metadata": {},
   "source": [
    "---\n",
    "## 3 ¬∑ Amostragem Estratificada (300.000 registros por `grade`)\n",
    "\n",
    "Com ~2,2 milh√µes de linhas, processar o dataset completo seria demorado e desnecess√°rio para o escopo deste projeto. Vamos extrair uma **amostra de 300.000 registros**.\n",
    "\n",
    "**Por que estratificada?** A coluna `grade` (A, B, C, D, E, F, G) representa a classifica√ß√£o de risco atribu√≠da pelo Lending Club. Se fiz√©ssemos amostragem aleat√≥ria simples, poder√≠amos sub-representar grades raras (F, G). A amostragem estratificada garante que **a propor√ß√£o de cada grade no sample seja igual √† do dataset original**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8752f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3. AMOSTRAGEM ESTRATIFICADA ‚Äî 300.000 REGISTROS\n",
    "# =============================================================================\n",
    "# Precisamos que a coluna 'grade' exista e n√£o tenha NaN para estratificar.\n",
    "# Primeiro verificamos; depois amostramos proporcionalmente.\n",
    "\n",
    "SAMPLE_SIZE = 300_000\n",
    "\n",
    "# Remover linhas onde 'grade' √© NaN (s√£o poucas, se existirem)\n",
    "df_raw = df_raw.dropna(subset=['grade'])\n",
    "\n",
    "# --- Verificar a distribui√ß√£o original de grade ---\n",
    "print(\"üìä Distribui√ß√£o original de 'grade':\")\n",
    "print(df_raw['grade'].value_counts(normalize=True).sort_index().apply(lambda x: f\"{x:.2%}\"))\n",
    "print(f\"\\n   Total de registros: {len(df_raw):,}\")\n",
    "\n",
    "# --- Calcular a fra√ß√£o necess√°ria para obter ~300k registros ---\n",
    "# frac = tamanho_desejado / tamanho_total\n",
    "frac = SAMPLE_SIZE / len(df_raw)\n",
    "\n",
    "# --- Amostragem estratificada ---\n",
    "# Agrupamos por grade e amostramos a mesma fra√ß√£o de cada grupo.\n",
    "# Isso preserva a propor√ß√£o relativa de cada grade.\n",
    "df_sample = (\n",
    "    df_raw\n",
    "    .groupby('grade', group_keys=False)\n",
    "    .apply(lambda x: x.sample(frac=frac, random_state=RANDOM_STATE))\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Amostra criada: {len(df_sample):,} registros\")\n",
    "\n",
    "# --- Comparar propor√ß√µes: original vs amostra ---\n",
    "comparison = pd.DataFrame({\n",
    "    'Original (%)': df_raw['grade'].value_counts(normalize=True).sort_index() * 100,\n",
    "    'Amostra (%)':  df_sample['grade'].value_counts(normalize=True).sort_index() * 100\n",
    "}).round(2)\n",
    "comparison['Diferen√ßa (pp)'] = (comparison['Amostra (%)'] - comparison['Original (%)']).round(2)\n",
    "print(\"\\nüìã Compara√ß√£o de propor√ß√µes (Original vs Amostra):\")\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba25ff02",
   "metadata": {},
   "source": [
    "---\n",
    "## 4 ¬∑ Sele√ß√£o de Vari√°veis e Engenharia de Features\n",
    "\n",
    "Selecionamos apenas as colunas relevantes e criamos a feature `fico_score`:\n",
    "\n",
    "| Vari√°vel | Descri√ß√£o |\n",
    "|---|---|\n",
    "| `int_rate` | **Vari√°vel-alvo** ‚Äî Taxa de juros (%) |\n",
    "| `annual_inc` | Renda anual declarada (USD) |\n",
    "| `dti` | Debt-to-Income ratio (%) ‚Äî raz√£o d√≠vida/renda |\n",
    "| `fico_range_low` / `fico_range_high` | Faixa do score FICO ‚Üí criamos `fico_score` = m√©dia |\n",
    "| `grade` | Classifica√ß√£o de risco (usada na amostragem; ser√° dropada) |\n",
    "\n",
    "> **Sobre o `fico_score`:** O FICO score √© fornecido em faixa (ex: 670‚Äì674). Calcular a **m√©dia aritm√©tica** das extremidades √© a aproxima√ß√£o padr√£o utilizada na literatura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16778d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4. SELE√á√ÉO DE VARI√ÅVEIS E ENGENHARIA DE FEATURES\n",
    "# =============================================================================\n",
    "\n",
    "# --- Selecionar apenas as colunas que precisamos ---\n",
    "# Isso reduz drasticamente o uso de mem√≥ria e simplifica o trabalho.\n",
    "colunas_relevantes = ['int_rate', 'annual_inc', 'dti', 'fico_range_low', 'fico_range_high', 'grade']\n",
    "df = df_sample[colunas_relevantes].copy()\n",
    "\n",
    "# --- Garantir que int_rate √© num√©rico ---\n",
    "# Em algumas vers√µes do dataset, int_rate vem como string com \"%\" (ex: \"13.56%\").\n",
    "# Precisamos remover o s√≠mbolo e converter para float.\n",
    "if df['int_rate'].dtype == 'object':\n",
    "    df['int_rate'] = df['int_rate'].str.replace('%', '', regex=False).astype(float)\n",
    "    print(\"‚ö†Ô∏è  int_rate estava como texto ‚Äî convertido para float (removido '%').\")\n",
    "else:\n",
    "    print(\"‚úÖ int_rate j√° est√° em formato num√©rico.\")\n",
    "\n",
    "# --- Criar a feature fico_score ---\n",
    "# O FICO score √© fornecido em faixa (ex: low=670, high=674).\n",
    "# A m√©dia aritm√©tica √© a aproxima√ß√£o padr√£o: fico_score = (low + high) / 2\n",
    "df['fico_score'] = (df['fico_range_low'] + df['fico_range_high']) / 2\n",
    "\n",
    "# --- Remover colunas auxiliares que n√£o ser√£o mais usadas ---\n",
    "df = df.drop(columns=['fico_range_low', 'fico_range_high', 'grade'])\n",
    "\n",
    "print(f\"\\n‚úÖ DataFrame final: {df.shape[0]:,} linhas √ó {df.shape[1]} colunas\")\n",
    "print(f\"   Colunas: {list(df.columns)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3a1bff",
   "metadata": {},
   "source": [
    "---\n",
    "## 5 ¬∑ Tratamento de Valores Ausentes\n",
    "\n",
    "Antes de qualquer an√°lise, precisamos verificar se h√° dados faltantes. Valores `NaN` podem:\n",
    "\n",
    "- **Enviesar os coeficientes** da regress√£o (se n√£o forem aleat√≥rios).\n",
    "- **Causar erros** em fun√ß√µes que n√£o aceitam NaN (ex: `shapiro()`, `OLS`).\n",
    "\n",
    "**Estrat√©gia adotada:** Primeiro, **analisamos o padr√£o de aus√™ncia** para entender se os dados faltantes carregam informa√ß√£o √∫til. S√≥ removemos NaN das colunas estritamente necess√°rias para as regress√µes (`int_rate`, `annual_inc`, `dti`, `fico_score`), pois o OLS exige dados completos. Para todas as demais colunas, os NaN s√£o **preservados** e analisados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65beb80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5. TRATAMENTO DE VALORES AUSENTES\n",
    "# =============================================================================\n",
    "\n",
    "# --- Diagn√≥stico: quantos NaN existem em cada coluna? ---\n",
    "missing = pd.DataFrame({\n",
    "    'Qtd. Ausentes': df.isnull().sum(),\n",
    "    '% Ausentes': (df.isnull().mean() * 100).round(2)\n",
    "})\n",
    "print(\"üìã Valores ausentes por coluna:\")\n",
    "print(missing)\n",
    "print(f\"\\n   Shape ANTES do tratamento: {df.shape}\")\n",
    "\n",
    "# --- An√°lise do padr√£o de aus√™ncia ---\n",
    "# Verificar se os NaN est√£o concentrados nas mesmas linhas ou distribu√≠dos\n",
    "nan_por_linha = df.isnull().sum(axis=1)\n",
    "print(f\"\\nüìä Distribui√ß√£o de NaN por linha:\")\n",
    "print(f\"   Linhas sem nenhum NaN:  {(nan_por_linha == 0).sum():,}\")\n",
    "print(f\"   Linhas com 1+ NaN:     {(nan_por_linha > 0).sum():,}\")\n",
    "print(f\"   Linhas com todos NaN:  {(nan_por_linha == df.shape[1]).sum():,}\")\n",
    "\n",
    "# --- Verificar se os NaN em cada coluna revelam algum padr√£o ---\n",
    "# Por exemplo: ser√° que linhas com NaN em 'dti' possuem int_rate diferente?\n",
    "colunas_regressao = ['int_rate', 'annual_inc', 'dti', 'fico_score']\n",
    "for col in colunas_regressao:\n",
    "    n_nan = df[col].isnull().sum()\n",
    "    if n_nan > 0:\n",
    "        media_com = df.loc[df[col].notna(), 'int_rate'].mean() if col != 'int_rate' else None\n",
    "        media_sem = df.loc[df[col].isna(), 'int_rate'].mean() if col != 'int_rate' and df.loc[df[col].isna(), 'int_rate'].notna().any() else None\n",
    "        print(f\"\\n   üîç {col}: {n_nan:,} NaN ({n_nan/len(df)*100:.2f}%)\")\n",
    "        if media_com is not None and media_sem is not None:\n",
    "            print(f\"      int_rate m√©dia (com {col}): {media_com:.2f}%\")\n",
    "            print(f\"      int_rate m√©dia (sem {col}): {media_sem:.2f}%\")\n",
    "            diff = abs(media_com - media_sem)\n",
    "            print(f\"      Diferen√ßa: {diff:.2f} pp {'‚ö†Ô∏è significativa' if diff > 1 else '‚úÖ pequena'}\")\n",
    "\n",
    "# --- Remover NaN SOMENTE nas colunas necess√°rias para a regress√£o ---\n",
    "# O OLS exige dados completos nas vari√°veis X e y. N√£o podemos regredir com NaN.\n",
    "# Justificativa: removemos APENAS onde os c√°lculos exigem.\n",
    "df = df.dropna(subset=colunas_regressao)\n",
    "\n",
    "print(f\"\\n   Shape DEPOIS da remo√ß√£o (apenas colunas de regress√£o): {df.shape}\")\n",
    "\n",
    "# --- Confirmar que as colunas de regress√£o n√£o t√™m NaN ---\n",
    "assert df[colunas_regressao].isnull().sum().sum() == 0, \"Ainda existem NaN nas colunas de regress√£o!\"\n",
    "print(\"\\n‚úÖ Nenhum valor ausente nas colunas de regress√£o.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf97c81",
   "metadata": {},
   "source": [
    "---\n",
    "## 6 ¬∑ An√°lise Explorat√≥ria (EDA) ‚Äî Estat√≠sticas Descritivas\n",
    "\n",
    "Antes de modelar, precisamos **entender os dados**. As estat√≠sticas descritivas nos revelam:\n",
    "\n",
    "- **Tend√™ncia central** (m√©dia, mediana) ‚Äî onde se concentram os valores.\n",
    "- **Dispers√£o** (desvio-padr√£o, min/max) ‚Äî qu√£o espalhados est√£o.\n",
    "- **Assimetria** ‚Äî vari√°veis como `annual_inc` tendem a ter cauda longa √† direita (poucos ganham muito).\n",
    "\n",
    "Esse entendimento guia decis√µes posteriores (ex: tratamento de outliers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38d71fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6. AN√ÅLISE EXPLORAT√ìRIA ‚Äî ESTAT√çSTICAS DESCRITIVAS\n",
    "# =============================================================================\n",
    "\n",
    "# --- Estat√≠sticas descritivas de todas as vari√°veis num√©ricas ---\n",
    "# O .describe() nos d√° count, mean, std, min, 25%, 50% (mediana), 75%, max.\n",
    "print(\"üìä Estat√≠sticas Descritivas:\")\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c9a09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tipos de dados e informa√ß√µes gerais ---\n",
    "print(\"üìã Informa√ß√µes do DataFrame:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60db2bb0",
   "metadata": {},
   "source": [
    "---\n",
    "## 7 ¬∑ EDA ‚Äî Distribui√ß√£o da Vari√°vel-Alvo (`int_rate`)\n",
    "\n",
    "Visualizamos a distribui√ß√£o da vari√°vel que queremos prever. \n",
    "\n",
    "> **Importante:** A **normalidade da vari√°vel-alvo N√ÉO √© uma premissa da regress√£o linear**. O que precisa ser normal s√£o os **res√≠duos** (erros). Por√©m, entender a distribui√ß√£o de `int_rate` nos ajuda a compreender o fen√¥meno e antecipar problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6472a873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7. DISTRIBUI√á√ÉO DA VARI√ÅVEL-ALVO (int_rate)\n",
    "# =============================================================================\n",
    "\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Histograma com curva KDE (Kernel Density Estimation)\n",
    "sns.histplot(df['int_rate'], kde=True, bins=50, color='steelblue', ax=ax)\n",
    "\n",
    "# Linhas verticais para m√©dia e mediana ‚Äî permite comparar tend√™ncia central\n",
    "media = df['int_rate'].mean()\n",
    "mediana = df['int_rate'].median()\n",
    "ax.axvline(media, color='red', linestyle='--', linewidth=2, label=f'M√©dia = {media:.2f}%')\n",
    "ax.axvline(mediana, color='green', linestyle='--', linewidth=2, label=f'Mediana = {mediana:.2f}%')\n",
    "\n",
    "ax.set_title('Distribui√ß√£o da Taxa de Juros (int_rate)', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Taxa de Juros (%)')\n",
    "ax.set_ylabel('Frequ√™ncia')\n",
    "ax.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- M√©tricas de forma da distribui√ß√£o ---\n",
    "sk = skew(df['int_rate'])\n",
    "kt = kurtosis(df['int_rate'])\n",
    "print(f\"üìà Assimetria (Skewness): {sk:.3f}\")\n",
    "print(f\"   ‚Üí {'Assim√©trica √† direita (positiva)' if sk > 0 else 'Assim√©trica √† esquerda' if sk < 0 else 'Sim√©trica'}\")\n",
    "print(f\"üìà Curtose (Kurtosis):    {kt:.3f}\")\n",
    "print(f\"   ‚Üí {'Leptoc√∫rtica (caudas pesadas)' if kt > 0 else 'Platic√∫rtica (caudas leves)' if kt < 0 else 'Mesoc√∫rtica'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3975d3e",
   "metadata": {},
   "source": [
    "---\n",
    "## 8 ¬∑ EDA ‚Äî Matriz de Correla√ß√£o (Heatmap)\n",
    "\n",
    "A **correla√ß√£o de Pearson** mede a for√ßa e dire√ß√£o da rela√ß√£o **linear** entre duas vari√°veis (varia de -1 a +1).\n",
    "\n",
    "**Expectativas baseadas no conhecimento de neg√≥cio:**\n",
    "- `fico_score` vs `int_rate`: **correla√ß√£o negativa forte** (maior score ‚Üí menor risco ‚Üí juros menores).\n",
    "- `dti` vs `int_rate`: **correla√ß√£o positiva** (maior DTI ‚Üí mais d√≠vida ‚Üí juros maiores).\n",
    "- `annual_inc` vs `int_rate`: **correla√ß√£o negativa fraca** (renda alta ‚â† necessariamente juros baixos, pois o FICO j√° captura muito do risco)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5b4c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 8. MATRIZ DE CORRELA√á√ÉO (HEATMAP)\n",
    "# =============================================================================\n",
    "\n",
    "# Calcular a matriz de correla√ß√£o de Pearson\n",
    "# Pearson mede APENAS rela√ß√µes lineares. Se a rela√ß√£o for n√£o-linear,\n",
    "# a correla√ß√£o pode ser baixa mesmo que haja uma associa√ß√£o forte.\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot=True,         # Mostrar os valores num√©ricos\n",
    "    fmt='.3f',          # 3 casas decimais\n",
    "    cmap='coolwarm',    # Paleta: azul (negativo) ‚Üí vermelho (positivo)\n",
    "    center=0,           # Centralizar a escala no zero\n",
    "    vmin=-1, vmax=1,    # Fixar limites\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_title('Matriz de Correla√ß√£o de Pearson', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Imprimir correla√ß√µes espec√≠ficas com int_rate ---\n",
    "print(\"üìä Correla√ß√£o de Pearson com int_rate:\")\n",
    "for col in ['annual_inc', 'dti', 'fico_score']:\n",
    "    r = corr_matrix.loc[col, 'int_rate']\n",
    "    forca = 'forte' if abs(r) > 0.5 else 'moderada' if abs(r) > 0.3 else 'fraca'\n",
    "    direcao = 'negativa' if r < 0 else 'positiva'\n",
    "    print(f\"   {col:15s} ‚Üí r = {r:+.4f} (correla√ß√£o {forca} {direcao})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e0ac1a",
   "metadata": {},
   "source": [
    "---\n",
    "## 9 ¬∑ EDA ‚Äî Scatter Plots (X vs `int_rate`)\n",
    "\n",
    "Visualizamos graficamente a rela√ß√£o entre cada vari√°vel independente e a taxa de juros. A **linearidade visual** √© a primeira premissa a ser verificada: se os pontos n√£o seguirem uma tend√™ncia linear, a regress√£o linear simples pode n√£o ser o modelo ideal.\n",
    "\n",
    "Usamos `alpha` baixo nos pontos porque temos milhares de observa√ß√µes ‚Äî sem transpar√™ncia, o gr√°fico ficaria uma massa s√≥lida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68adcaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 9. SCATTER PLOTS ‚Äî X vs int_rate\n",
    "# =============================================================================\n",
    "\n",
    "variaveis = {\n",
    "    'annual_inc': 'Renda Anual (USD)',\n",
    "    'dti': 'Debt-to-Income Ratio (%)',\n",
    "    'fico_score': 'Score FICO'\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "for ax, (col, label) in zip(axes, variaveis.items()):\n",
    "    # regplot = scatter + linha de regress√£o com intervalo de confian√ßa\n",
    "    sns.regplot(\n",
    "        x=col, y='int_rate', data=df,\n",
    "        ax=ax,\n",
    "        scatter_kws={'alpha': 0.1, 's': 5, 'color': 'steelblue'},  # alpha baixo para volume grande\n",
    "        line_kws={'color': 'red', 'linewidth': 2},\n",
    "        ci=95  # Intervalo de confian√ßa de 95%\n",
    "    )\n",
    "    ax.set_title(f'{label} vs Taxa de Juros', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel(label)\n",
    "    ax.set_ylabel('Taxa de Juros (%)')\n",
    "\n",
    "plt.suptitle('Scatter Plots ‚Äî Vari√°veis Independentes vs int_rate', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6be29b6",
   "metadata": {},
   "source": [
    "---\n",
    "## 10 ¬∑ Detec√ß√£o e Remo√ß√£o de Outliers (M√©todo IQR)\n",
    "\n",
    "**Outliers s√£o perigosos para a regress√£o linear** porque o OLS (Ordinary Least Squares) minimiza a **soma dos quadrados** dos res√≠duos. Valores extremos geram res√≠duos enormes que, ao serem elevados ao quadrado, dominam a fun√ß√£o de custo e \"puxam\" a reta na dire√ß√£o deles.\n",
    "\n",
    "**M√©todo utilizado ‚Äî IQR (Interquartile Range):**\n",
    "- $IQR = Q3 - Q1$ (dist√¢ncia entre o 3¬∫ e o 1¬∫ quartil)\n",
    "- Limite inferior: $Q1 - 1.5 \\times IQR$\n",
    "- Limite superior: $Q3 + 1.5 \\times IQR$\n",
    "- Qualquer valor fora desses limites √© considerado outlier.\n",
    "\n",
    "Este √© o mesmo crit√©rio usado em boxplots. √â robusto e amplamente aceito na literatura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c14cace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 10. DETEC√á√ÉO E REMO√á√ÉO DE OUTLIERS ‚Äî M√âTODO IQR\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"üìä Shape ANTES da remo√ß√£o de outliers: {df.shape}\")\n",
    "\n",
    "# --- Boxplots ANTES ---\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "for ax, col in zip(axes, ['int_rate', 'annual_inc', 'dti', 'fico_score']):\n",
    "    sns.boxplot(y=df[col], ax=ax, color='salmon')\n",
    "    ax.set_title(f'{col} (ANTES)', fontweight='bold')\n",
    "plt.suptitle('Boxplots ANTES da Remo√ß√£o de Outliers', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Fun√ß√£o para remo√ß√£o de outliers via IQR ---\n",
    "def remover_outliers_iqr(dataframe, coluna):\n",
    "    \"\"\"\n",
    "    Remove outliers de uma coluna usando o m√©todo IQR (Interquartile Range).\n",
    "    \n",
    "    Matematicamente:\n",
    "        - IQR = Q3 - Q1\n",
    "        - Limite inferior = Q1 - 1.5 * IQR\n",
    "        - Limite superior = Q3 + 1.5 * IQR\n",
    "    \n",
    "    Retorna o DataFrame filtrado e a quantidade de outliers removidos.\n",
    "    \"\"\"\n",
    "    Q1 = dataframe[coluna].quantile(0.25)\n",
    "    Q3 = dataframe[coluna].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    \n",
    "    antes = len(dataframe)\n",
    "    dataframe = dataframe[(dataframe[coluna] >= lower) & (dataframe[coluna] <= upper)]\n",
    "    removidos = antes - len(dataframe)\n",
    "    \n",
    "    print(f\"   {coluna:15s} ‚Üí Limites [{lower:.2f}, {upper:.2f}] | Removidos: {removidos:,}\")\n",
    "    return dataframe\n",
    "\n",
    "# --- Aplicar para cada vari√°vel ---\n",
    "print(\"\\nüîß Removendo outliers por vari√°vel:\")\n",
    "for col in ['int_rate', 'annual_inc', 'dti', 'fico_score']:\n",
    "    df = remover_outliers_iqr(df, col)\n",
    "\n",
    "print(f\"\\n‚úÖ Shape DEPOIS da remo√ß√£o de outliers: {df.shape}\")\n",
    "\n",
    "# --- Boxplots DEPOIS ---\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "for ax, col in zip(axes, ['int_rate', 'annual_inc', 'dti', 'fico_score']):\n",
    "    sns.boxplot(y=df[col], ax=ax, color='lightgreen')\n",
    "    ax.set_title(f'{col} (DEPOIS)', fontweight='bold')\n",
    "plt.suptitle('Boxplots DEPOIS da Remo√ß√£o de Outliers', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca1496c",
   "metadata": {},
   "source": [
    "---\n",
    "## 11 ¬∑ Divis√£o dos Dados ‚Äî Train/Test Split (80/20)\n",
    "\n",
    "Dividimos os dados em **treino (80%)** e **teste (20%)** para cada par (X, y):\n",
    "\n",
    "- **Treino:** usado para estimar os coeficientes da regress√£o (Œ≤‚ÇÄ, Œ≤‚ÇÅ).\n",
    "- **Teste:** usado para avaliar o modelo em dados que ele **nunca viu** (preven√ß√£o de overfitting).\n",
    "\n",
    "A divis√£o √© feita via **permuta√ß√£o aleat√≥ria com NumPy**, garantindo reprodutibilidade (seed fixa).\n",
    "\n",
    "> ‚ö†Ô∏è **A divis√£o √© feita AP√ìS a remo√ß√£o de outliers** para evitar *data leakage* (contamina√ß√£o do teste com informa√ß√µes do pr√©-processamento)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea122659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 11. DIVIS√ÉO DOS DADOS ‚Äî TRAIN/TEST SPLIT (80/20)\n",
    "# =============================================================================\n",
    "\n",
    "# Vari√°vel alvo (y) ‚Äî a mesma para os 3 modelos\n",
    "y = df['int_rate'].values\n",
    "\n",
    "# --- Modelo 1: annual_inc ‚Üí int_rate ---\n",
    "X1 = df[['annual_inc']].values  # Formato 2D\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split_manual(\n",
    "    X1, y, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# --- Modelo 2: dti ‚Üí int_rate ---\n",
    "X2 = df[['dti']].values\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split_manual(\n",
    "    X2, y, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# --- Modelo 3: fico_score ‚Üí int_rate ---\n",
    "X3 = df[['fico_score']].values\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split_manual(\n",
    "    X3, y, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# --- Verificar shapes ---\n",
    "print(\"üìã Shapes dos conjuntos de dados:\")\n",
    "print(f\"{'Modelo':<12} {'X_train':>10} {'X_test':>10} {'y_train':>10} {'y_test':>10}\")\n",
    "print(\"-\" * 55)\n",
    "for nome, xt, xte, yt, yte in [\n",
    "    ('annual_inc', X1_train, X1_test, y1_train, y1_test),\n",
    "    ('dti',        X2_train, X2_test, y2_train, y2_test),\n",
    "    ('fico_score', X3_train, X3_test, y3_train, y3_test),\n",
    "]:\n",
    "    print(f\"{nome:<12} {str(xt.shape):>10} {str(xte.shape):>10} {str(yt.shape):>10} {str(yte.shape):>10}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Dados divididos: 80% treino / 20% teste (random_state={RANDOM_STATE})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff48b38",
   "metadata": {},
   "source": [
    "---\n",
    "# üî∑ MODELO 1: `annual_inc` ‚Üí `int_rate`\n",
    "## Renda Anual como Preditora da Taxa de Juros\n",
    "\n",
    "**Hip√≥tese de neg√≥cio:** Clientes com **maior renda anual** representam menor risco de inadimpl√™ncia, portanto o banco deveria cobrar **juros menores**. Esperamos um coeficiente Œ≤‚ÇÅ **negativo**.\n",
    "\n",
    "**Equa√ß√£o do modelo:**\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1 \\cdot \\text{annual\\_inc}$$\n",
    "\n",
    "Onde:\n",
    "- $\\hat{y}$ = taxa de juros prevista (%)\n",
    "- $\\beta_0$ = intercepto (juros quando renda = 0, interpreta√ß√£o te√≥rica)\n",
    "- $\\beta_1$ = varia√ß√£o na taxa de juros para cada d√≥lar a mais de renda\n",
    "\n",
    "---\n",
    "\n",
    "### 12 ¬∑ Modelo 1: Regress√£o OLS (statsmodels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcb67f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 12. MODELO 1 ‚Äî annual_inc ‚Üí int_rate (STATSMODELS OLS)\n",
    "# =============================================================================\n",
    "# O statsmodels exige que adicionemos uma coluna de constante (1's) ao X para\n",
    "# que o modelo estime o intercepto (Œ≤‚ÇÄ). Sem isso, a reta seria for√ßada a\n",
    "# passar pela origem, o que n√£o faz sentido para nosso problema.\n",
    "\n",
    "X1_train_sm = sm.add_constant(X1_train)  # Adiciona coluna de 1's\n",
    "X1_test_sm  = sm.add_constant(X1_test)\n",
    "\n",
    "# --- Ajustar o modelo OLS ---\n",
    "# OLS = Ordinary Least Squares (M√≠nimos Quadrados Ordin√°rios)\n",
    "# √â o m√©todo que encontra Œ≤‚ÇÄ e Œ≤‚ÇÅ minimizando a soma dos quadrados dos res√≠duos:\n",
    "# min Œ£(y·µ¢ - ≈∑·µ¢)¬≤ = min Œ£(y·µ¢ - Œ≤‚ÇÄ - Œ≤‚ÇÅ¬∑x·µ¢)¬≤\n",
    "modelo1_ols = sm.OLS(y1_train, X1_train_sm).fit()\n",
    "\n",
    "# --- Exibir o sum√°rio estat√≠stico completo ---\n",
    "# Este √© o \"cora√ß√£o\" da an√°lise estat√≠stica: p-values, R¬≤, F-statistic, etc.\n",
    "print(\"=\" * 70)\n",
    "print(\"MODELO 1: annual_inc ‚Üí int_rate (OLS Summary)\")\n",
    "print(\"=\" * 70)\n",
    "print(modelo1_ols.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a64b910",
   "metadata": {},
   "source": [
    "### 13 ¬∑ Modelo 1: Previs√µes e M√©tricas de Avalia√ß√£o\n",
    "\n",
    "Usando os coeficientes estimados pelo OLS, fazemos previs√µes no conjunto de teste e calculamos as m√©tricas R¬≤, RMSE e MAE manualmente com NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836b903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 13. MODELO 1 ‚Äî annual_inc ‚Üí int_rate (PREVIS√ïES E M√âTRICAS)\n",
    "# =============================================================================\n",
    "\n",
    "# --- Coeficientes do OLS ---\n",
    "beta0_m1 = modelo1_ols.params[0]  # Intercepto\n",
    "beta1_m1 = modelo1_ols.params[1]  # Coeficiente de annual_inc\n",
    "\n",
    "print(\"üìê Coeficientes (OLS statsmodels):\")\n",
    "print(f\"   Œ≤‚ÇÄ (intercepto):  {beta0_m1:.6f}\")\n",
    "print(f\"   Œ≤‚ÇÅ (annual_inc):  {beta1_m1:.10f}\")\n",
    "\n",
    "# --- Previs√µes no conjunto de teste ---\n",
    "y1_pred = modelo1_ols.predict(X1_test_sm)\n",
    "\n",
    "# --- M√©tricas de avalia√ß√£o (c√°lculo manual) ---\n",
    "r2_m1   = r2_score_manual(y1_test, y1_pred)\n",
    "rmse_m1 = rmse_manual(y1_test, y1_pred)\n",
    "mae_m1  = mae_manual(y1_test, y1_pred)\n",
    "\n",
    "print(f\"\\nüìä M√©tricas no Conjunto de Teste:\")\n",
    "print(f\"   R¬≤ (Coef. Determina√ß√£o): {r2_m1:.6f}\")\n",
    "print(f\"   RMSE:                    {rmse_m1:.4f}%\")\n",
    "print(f\"   MAE:                     {mae_m1:.4f}%\")\n",
    "\n",
    "# --- Scatter Plot com reta de regress√£o ---\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.scatter(X1_test, y1_test, alpha=0.15, s=8, color='steelblue', label='Dados reais (teste)')\n",
    "# Reta: ordenamos X_test para que a linha seja cont√≠nua\n",
    "sorted_idx = X1_test.flatten().argsort()\n",
    "ax.plot(X1_test.flatten()[sorted_idx], y1_pred[sorted_idx],\n",
    "        color='red', linewidth=2, label='Reta de Regress√£o')\n",
    "ax.set_title('Modelo 1: annual_inc ‚Üí int_rate (OLS)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Renda Anual (USD)')\n",
    "ax.set_ylabel('Taxa de Juros (%)')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff943e6",
   "metadata": {},
   "source": [
    "### 14 ¬∑ Modelo 1: Rede Neural Simples (TensorFlow/Keras)\n",
    "\n",
    "Constru√≠mos uma rede neural com **um √∫nico neur√¥nio linear** para demonstrar a equival√™ncia matem√°tica:\n",
    "\n",
    "$$\\text{Rede Neural:} \\quad \\hat{y} = W \\cdot x + b$$\n",
    "$$\\text{Regress√£o Linear:} \\quad \\hat{y} = \\beta_1 \\cdot x + \\beta_0$$\n",
    "\n",
    "S√£o a **mesma equa√ß√£o**! O neur√¥nio aprende $W \\approx \\beta_1$ e $b \\approx \\beta_0$ via gradient descent.\n",
    "\n",
    "> **Padroniza√ß√£o:** Redes neurais convergem **muito melhor** quando os dados est√£o na mesma escala. Usamos padroniza√ß√£o z-score ($z = \\frac{x - \\mu}{\\sigma}$) antes de treinar. Depois, desnormalizamos os pesos para comparar com os coeficientes da regress√£o cl√°ssica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d342cabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 14. MODELO 1 ‚Äî annual_inc ‚Üí int_rate (REDE NEURAL - TENSORFLOW/KERAS)\n",
    "# =============================================================================\n",
    "\n",
    "# --- Padroniza√ß√£o z-score manual ---\n",
    "# A rede neural √© sens√≠vel √† escala dos dados. annual_inc est√° na faixa de\n",
    "# dezenas/centenas de milhares, enquanto int_rate est√° entre 5 e 25.\n",
    "# Sem padroniza√ß√£o, o gradiente pode explodir ou convergir muito lentamente.\n",
    "X1_train_scaled, X1_test_scaled, mean1, std1 = standard_scale(X1_train, X1_test)\n",
    "\n",
    "# --- Construir a rede neural ---\n",
    "# Arquitetura: 1 neur√¥nio de entrada ‚Üí 1 neur√¥nio de sa√≠da (linear)\n",
    "# Isso √© ALGEBRICAMENTE ID√äNTICO √† regress√£o linear: ≈∑ = W¬∑x + b\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "\n",
    "modelo1_nn = Sequential([\n",
    "    Dense(1, input_shape=(1,), activation='linear')  # 1 neur√¥nio, ativa√ß√£o linear\n",
    "])\n",
    "\n",
    "# --- Compilar ---\n",
    "# Optimizer: Adam (variante eficiente do gradient descent)\n",
    "# Loss: MSE (Mean Squared Error) ‚Äî mesma fun√ß√£o de custo do OLS!\n",
    "modelo1_nn.compile(\n",
    "    optimizer=Adam(learning_rate=0.01),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# --- Treinar ---\n",
    "print(\"‚è≥ Treinando Rede Neural ‚Äî Modelo 1 (annual_inc ‚Üí int_rate)...\")\n",
    "history1 = modelo1_nn.fit(\n",
    "    X1_train_scaled, y1_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,  # 10% do treino para valida√ß√£o interna\n",
    "    verbose=0  # Silencioso (mostramos a curva de loss depois)\n",
    ")\n",
    "print(\"‚úÖ Treino conclu√≠do!\")\n",
    "\n",
    "# --- Curvas de Loss (Treino vs Valida√ß√£o) ---\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(history1.history['loss'], label='Loss (Treino)', linewidth=2)\n",
    "ax.plot(history1.history['val_loss'], label='Loss (Valida√ß√£o)', linewidth=2, linestyle='--')\n",
    "ax.set_title('Modelo 1 (NN): Curva de Loss por √âpoca', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('√âpoca')\n",
    "ax.set_ylabel('MSE (Mean Squared Error)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Extrair e desnormalizar pesos ---\n",
    "# Os pesos da rede foram aprendidos no espa√ßo padronizado (z-score).\n",
    "# Para comparar com Œ≤‚ÇÅ e Œ≤‚ÇÄ do OLS, precisamos desnormalizar:\n",
    "#   Œ≤‚ÇÅ_real = W / œÉ_x\n",
    "#   Œ≤‚ÇÄ_real = b - W * Œº_x / œÉ_x\n",
    "W_scaled, b_scaled = modelo1_nn.get_weights()\n",
    "W_real = W_scaled[0][0] / std1[0]\n",
    "b_real = b_scaled[0] - W_scaled[0][0] * mean1[0] / std1[0]\n",
    "\n",
    "print(f\"\\nüìê Pesos da Rede Neural (desnormalizados):\")\n",
    "print(f\"   W (‚âà Œ≤‚ÇÅ): {W_real:.10f}\")\n",
    "print(f\"   b (‚âà Œ≤‚ÇÄ): {b_real:.6f}\")\n",
    "print(f\"\\n   Compara√ß√£o com OLS (statsmodels):\")\n",
    "print(f\"   Œ≤‚ÇÅ (OLS): {beta1_m1:.10f}\")\n",
    "print(f\"   Œ≤‚ÇÄ (OLS): {beta0_m1:.6f}\")\n",
    "\n",
    "# --- M√©tricas no conjunto de teste ---\n",
    "y1_pred_nn = modelo1_nn.predict(X1_test_scaled, verbose=0).flatten()\n",
    "r2_m1_nn   = r2_score_manual(y1_test, y1_pred_nn)\n",
    "rmse_m1_nn = rmse_manual(y1_test, y1_pred_nn)\n",
    "mae_m1_nn  = mae_manual(y1_test, y1_pred_nn)\n",
    "\n",
    "print(f\"\\nüìä M√©tricas (Rede Neural) no Conjunto de Teste:\")\n",
    "print(f\"   R¬≤:   {r2_m1_nn:.6f}\")\n",
    "print(f\"   RMSE: {rmse_m1_nn:.4f}%\")\n",
    "print(f\"   MAE:  {mae_m1_nn:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d832146",
   "metadata": {},
   "source": [
    "### 15 ¬∑ Modelo 1: Valida√ß√£o das Premissas ‚Äî Linearidade e Homocedasticidade\n",
    "\n",
    "As duas premissas mais importantes da regress√£o OLS que podemos verificar visualmente:\n",
    "\n",
    "1. **Linearidade:** A rela√ß√£o entre X e Y deve ser linear. No gr√°fico de res√≠duos vs X, **n√£o deve haver padr√£o curvil√≠neo**.\n",
    "2. **Homocedasticidade:** A vari√¢ncia dos res√≠duos deve ser **constante** ao longo de ≈∑. No gr√°fico de res√≠duos vs ≈∑, os pontos devem estar distribu√≠dos aleatoriamente, **sem formato de funil** (cone aberto = heterocedasticidade).\n",
    "\n",
    "> ‚ö†Ô∏è Se houver **heterocedasticidade**, os erros-padr√£o dos coeficientes ficam enviesados ‚Üí os p-values deixam de ser confi√°veis ‚Üí conclus√µes sobre signific√¢ncia estat√≠stica podem estar erradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620df4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 15. MODELO 1 ‚Äî VALIDA√á√ÉO: LINEARIDADE E HOMOCEDASTICIDADE\n",
    "# =============================================================================\n",
    "\n",
    "# Calcular res√≠duos: diferen√ßa entre o valor real e o previsto\n",
    "# Se o modelo fosse perfeito, todos os res√≠duos seriam zero.\n",
    "residuos1 = y1_test - y1_pred\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# --- Plot 1: Res√≠duos vs Valores Previstos (≈∑) ‚Üí Homocedasticidade ---\n",
    "axes[0].scatter(y1_pred, residuos1, alpha=0.15, s=8, color='steelblue')\n",
    "axes[0].axhline(y=0, color='red', linewidth=2, linestyle='--')\n",
    "axes[0].set_title('Res√≠duos vs Valores Previstos (≈∑)\\nHomocedasticidade', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('Valores Previstos (≈∑) ‚Äî Taxa de Juros (%)')\n",
    "axes[0].set_ylabel('Res√≠duos')\n",
    "\n",
    "# --- Plot 2: Res√≠duos vs X (annual_inc) ‚Üí Linearidade ---\n",
    "axes[1].scatter(X1_test, residuos1, alpha=0.15, s=8, color='coral')\n",
    "axes[1].axhline(y=0, color='red', linewidth=2, linestyle='--')\n",
    "axes[1].set_title('Res√≠duos vs annual_inc\\nLinearidade', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Renda Anual (USD)')\n",
    "axes[1].set_ylabel('Res√≠duos')\n",
    "\n",
    "plt.suptitle('Modelo 1 ‚Äî An√°lise de Res√≠duos', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2877607e",
   "metadata": {},
   "source": [
    "### 16 ¬∑ Modelo 1: Valida√ß√£o das Premissas ‚Äî Normalidade dos Res√≠duos\n",
    "\n",
    "A premissa de **normalidade dos res√≠duos** garante que os intervalos de confian√ßa e os p-values s√£o v√°lidos.\n",
    "\n",
    "Ferramentas utilizadas:\n",
    "- **Q-Q Plot** (Quantile-Quantile): se os pontos seguirem a linha de 45¬∞, os res√≠duos s√£o normais.\n",
    "- **Teste de Shapiro-Wilk**: teste formal ‚Äî $H_0$: os dados s√£o normalmente distribu√≠dos. Se $p > 0.05$, n√£o rejeitamos $H_0$.\n",
    "\n",
    "> **Aten√ß√£o:** Com $n$ grande (>5000), Shapiro-Wilk tende a rejeitar $H_0$ por ser muito sens√≠vel a pequenos desvios. Nesse caso, o **Q-Q Plot visual** e o **Teorema Central do Limite** s√£o argumentos mais convincentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904d1945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 16. MODELO 1 ‚Äî VALIDA√á√ÉO: NORMALIDADE DOS RES√çDUOS\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# --- Q-Q Plot ---\n",
    "# Se os res√≠duos forem normais, os pontos se alinham √† diagonal vermelha.\n",
    "qqplot(residuos1, line='45', ax=axes[0], markersize=2, alpha=0.3)\n",
    "axes[0].set_title('Q-Q Plot dos Res√≠duos\\n(Modelo 1: annual_inc)', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# --- Histograma dos Res√≠duos com KDE ---\n",
    "sns.histplot(residuos1, kde=True, bins=50, color='steelblue', ax=axes[1])\n",
    "axes[1].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1].set_title('Distribui√ß√£o dos Res√≠duos\\n(Modelo 1: annual_inc)', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Res√≠duo')\n",
    "axes[1].set_ylabel('Frequ√™ncia')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Teste de Shapiro-Wilk ---\n",
    "# O teste aceita no m√°ximo 5000 observa√ß√µes. Se temos mais, usamos uma sub-amostra.\n",
    "amostra_residuos1 = np.random.choice(residuos1, size=min(5000, len(residuos1)), replace=False)\n",
    "stat_sw, p_sw = shapiro(amostra_residuos1)\n",
    "\n",
    "print(f\"üìä Teste de Shapiro-Wilk (Modelo 1):\")\n",
    "print(f\"   Estat√≠stica W = {stat_sw:.6f}\")\n",
    "print(f\"   p-value       = {p_sw:.6e}\")\n",
    "if p_sw > 0.05:\n",
    "    print(\"   ‚úÖ N√£o rejeitamos H‚ÇÄ: os res√≠duos seguem distribui√ß√£o normal (p > 0.05)\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Rejeitamos H‚ÇÄ: desvio da normalidade detectado (p < 0.05)\")\n",
    "    print(\"   ‚Üí Com n grande, isso √© esperado (Shapiro-Wilk √© muito sens√≠vel).\")\n",
    "    print(\"   ‚Üí O Q-Q Plot visual e o Teorema Central do Limite justificam a validade.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e541c93d",
   "metadata": {},
   "source": [
    "### 17 ¬∑ Modelo 1: Valida√ß√£o das Premissas ‚Äî Independ√™ncia (Durbin-Watson)\n",
    "\n",
    "A estat√≠stica de **Durbin-Watson** testa se os res√≠duos s√£o **autocorrelacionados** (se o erro de uma observa√ß√£o influencia o erro da pr√≥xima).\n",
    "\n",
    "| Valor DW | Interpreta√ß√£o |\n",
    "|---|---|\n",
    "| ‚âà 2.0 | ‚úÖ Sem autocorrela√ß√£o |\n",
    "| ‚âà 0.0 | ‚ö†Ô∏è Autocorrela√ß√£o positiva forte |\n",
    "| ‚âà 4.0 | ‚ö†Ô∏è Autocorrela√ß√£o negativa forte |\n",
    "\n",
    "> Em dados **cross-section** (n√£o temporais), a autocorrela√ß√£o √© menos comum. Mas verificamos por boa pr√°tica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5b0a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 17. MODELO 1 ‚Äî VALIDA√á√ÉO: INDEPEND√äNCIA DOS RES√çDUOS (DURBIN-WATSON)\n",
    "# =============================================================================\n",
    "\n",
    "# Calcular Durbin-Watson\n",
    "# Usamos os res√≠duos do OLS no conjunto de treino (pr√°tica padr√£o com statsmodels)\n",
    "dw1 = durbin_watson(modelo1_ols.resid)\n",
    "\n",
    "print(f\"üìä Estat√≠stica de Durbin-Watson (Modelo 1): {dw1:.4f}\")\n",
    "if 1.5 < dw1 < 2.5:\n",
    "    print(\"   ‚úÖ Valor pr√≥ximo de 2 ‚Üí N√£o h√° evid√™ncia de autocorrela√ß√£o significativa.\")\n",
    "elif dw1 <= 1.5:\n",
    "    print(\"   ‚ö†Ô∏è  Valor baixo ‚Üí Poss√≠vel autocorrela√ß√£o positiva.\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Valor alto ‚Üí Poss√≠vel autocorrela√ß√£o negativa.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dc2a9f",
   "metadata": {},
   "source": [
    "---\n",
    "# üî∂ MODELO 2: `dti` ‚Üí `int_rate`\n",
    "## Debt-to-Income Ratio como Preditor da Taxa de Juros\n",
    "\n",
    "**Hip√≥tese de neg√≥cio:** Quanto **maior o DTI** (mais endividado em rela√ß√£o √† renda), maior o risco de inadimpl√™ncia ‚Üí o banco cobra **juros mais altos**. Esperamos Œ≤‚ÇÅ **positivo**.\n",
    "\n",
    "**Equa√ß√£o do modelo:**\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1 \\cdot \\text{dti}$$\n",
    "\n",
    "---\n",
    "\n",
    "### 18 ¬∑ Modelo 2: Regress√£o OLS (statsmodels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9711c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 18. MODELO 2 ‚Äî dti ‚Üí int_rate (STATSMODELS OLS)\n",
    "# =============================================================================\n",
    "\n",
    "X2_train_sm = sm.add_constant(X2_train)\n",
    "X2_test_sm  = sm.add_constant(X2_test)\n",
    "\n",
    "# Ajustar OLS\n",
    "modelo2_ols = sm.OLS(y2_train, X2_train_sm).fit()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODELO 2: dti ‚Üí int_rate (OLS Summary)\")\n",
    "print(\"=\" * 70)\n",
    "print(modelo2_ols.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0b9419",
   "metadata": {},
   "source": [
    "### 19 ¬∑ Modelo 2: Previs√µes e M√©tricas de Avalia√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb92f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 19. MODELO 2 ‚Äî dti ‚Üí int_rate (PREVIS√ïES E M√âTRICAS)\n",
    "# =============================================================================\n",
    "\n",
    "# Coeficientes do OLS\n",
    "beta0_m2 = modelo2_ols.params[0]\n",
    "beta1_m2 = modelo2_ols.params[1]\n",
    "\n",
    "print(\"üìê Coeficientes (OLS statsmodels):\")\n",
    "print(f\"   Œ≤‚ÇÄ (intercepto): {beta0_m2:.6f}\")\n",
    "print(f\"   Œ≤‚ÇÅ (dti):        {beta1_m2:.6f}\")\n",
    "\n",
    "# Previs√µes\n",
    "y2_pred = modelo2_ols.predict(X2_test_sm)\n",
    "\n",
    "# M√©tricas\n",
    "r2_m2   = r2_score_manual(y2_test, y2_pred)\n",
    "rmse_m2 = rmse_manual(y2_test, y2_pred)\n",
    "mae_m2  = mae_manual(y2_test, y2_pred)\n",
    "\n",
    "print(f\"\\nüìä M√©tricas no Conjunto de Teste:\")\n",
    "print(f\"   R¬≤:   {r2_m2:.6f}\")\n",
    "print(f\"   RMSE: {rmse_m2:.4f}%\")\n",
    "print(f\"   MAE:  {mae_m2:.4f}%\")\n",
    "\n",
    "# --- Scatter Plot com reta ---\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.scatter(X2_test, y2_test, alpha=0.15, s=8, color='darkorange', label='Dados reais (teste)')\n",
    "sorted_idx = X2_test.flatten().argsort()\n",
    "ax.plot(X2_test.flatten()[sorted_idx], y2_pred[sorted_idx],\n",
    "        color='darkred', linewidth=2, label='Reta de Regress√£o')\n",
    "ax.set_title('Modelo 2: dti ‚Üí int_rate (OLS)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Debt-to-Income Ratio (%)')\n",
    "ax.set_ylabel('Taxa de Juros (%)')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee872e8",
   "metadata": {},
   "source": [
    "### 20 ¬∑ Modelo 2: Rede Neural Simples (TensorFlow/Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bc98da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 20. MODELO 2 ‚Äî dti ‚Üí int_rate (REDE NEURAL - TENSORFLOW/KERAS)\n",
    "# =============================================================================\n",
    "\n",
    "# Padronizar dti (z-score manual)\n",
    "X2_train_scaled, X2_test_scaled, mean2, std2 = standard_scale(X2_train, X2_test)\n",
    "\n",
    "# Construir rede neural ‚Äî mesma arquitetura (1 neur√¥nio linear)\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "modelo2_nn = Sequential([\n",
    "    Dense(1, input_shape=(1,), activation='linear')\n",
    "])\n",
    "modelo2_nn.compile(optimizer=Adam(learning_rate=0.01), loss='mse', metrics=['mae'])\n",
    "\n",
    "# Treinar\n",
    "print(\"‚è≥ Treinando Rede Neural ‚Äî Modelo 2 (dti ‚Üí int_rate)...\")\n",
    "history2 = modelo2_nn.fit(\n",
    "    X2_train_scaled, y2_train,\n",
    "    epochs=100, batch_size=32,\n",
    "    validation_split=0.1, verbose=0\n",
    ")\n",
    "print(\"‚úÖ Treino conclu√≠do!\")\n",
    "\n",
    "# Curva de Loss\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(history2.history['loss'], label='Loss (Treino)', linewidth=2)\n",
    "ax.plot(history2.history['val_loss'], label='Loss (Valida√ß√£o)', linewidth=2, linestyle='--')\n",
    "ax.set_title('Modelo 2 (NN): Curva de Loss por √âpoca', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('√âpoca')\n",
    "ax.set_ylabel('MSE')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Desnormalizar pesos\n",
    "W2_scaled, b2_scaled = modelo2_nn.get_weights()\n",
    "W2_real = W2_scaled[0][0] / std2[0]\n",
    "b2_real = b2_scaled[0] - W2_scaled[0][0] * mean2[0] / std2[0]\n",
    "print(f\"\\nüìê Pesos da Rede Neural (desnormalizados):\")\n",
    "print(f\"   W (‚âà Œ≤‚ÇÅ): {W2_real:.6f}\")\n",
    "print(f\"   b (‚âà Œ≤‚ÇÄ): {b2_real:.6f}\")\n",
    "print(f\"   Compara√ß√£o OLS ‚Üí Œ≤‚ÇÅ: {beta1_m2:.6f}, Œ≤‚ÇÄ: {beta0_m2:.6f}\")\n",
    "\n",
    "# M√©tricas\n",
    "y2_pred_nn = modelo2_nn.predict(X2_test_scaled, verbose=0).flatten()\n",
    "r2_m2_nn   = r2_score_manual(y2_test, y2_pred_nn)\n",
    "rmse_m2_nn = rmse_manual(y2_test, y2_pred_nn)\n",
    "mae_m2_nn  = mae_manual(y2_test, y2_pred_nn)\n",
    "print(f\"\\nüìä M√©tricas (Rede Neural):\")\n",
    "print(f\"   R¬≤:   {r2_m2_nn:.6f}\")\n",
    "print(f\"   RMSE: {rmse_m2_nn:.4f}%\")\n",
    "print(f\"   MAE:  {mae_m2_nn:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69787a6",
   "metadata": {},
   "source": [
    "### 21 ¬∑ Modelo 2: Valida√ß√£o das Premissas ‚Äî Linearidade e Homocedasticidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6097e261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 21. MODELO 2 ‚Äî VALIDA√á√ÉO: LINEARIDADE E HOMOCEDASTICIDADE\n",
    "# =============================================================================\n",
    "\n",
    "residuos2 = y2_test - y2_pred\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Res√≠duos vs Valores Previstos ‚Üí Homocedasticidade\n",
    "axes[0].scatter(y2_pred, residuos2, alpha=0.15, s=8, color='darkorange')\n",
    "axes[0].axhline(y=0, color='red', linewidth=2, linestyle='--')\n",
    "axes[0].set_title('Res√≠duos vs Valores Previstos (≈∑)\\nHomocedasticidade', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('Valores Previstos (≈∑)')\n",
    "axes[0].set_ylabel('Res√≠duos')\n",
    "\n",
    "# Res√≠duos vs X ‚Üí Linearidade\n",
    "axes[1].scatter(X2_test, residuos2, alpha=0.15, s=8, color='coral')\n",
    "axes[1].axhline(y=0, color='red', linewidth=2, linestyle='--')\n",
    "axes[1].set_title('Res√≠duos vs dti\\nLinearidade', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('DTI (%)')\n",
    "axes[1].set_ylabel('Res√≠duos')\n",
    "\n",
    "plt.suptitle('Modelo 2 ‚Äî An√°lise de Res√≠duos', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219776b2",
   "metadata": {},
   "source": [
    "### 22 ¬∑ Modelo 2: Valida√ß√£o das Premissas ‚Äî Normalidade dos Res√≠duos (Q-Q Plot + Shapiro-Wilk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1169e0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 22. MODELO 2 ‚Äî VALIDA√á√ÉO: NORMALIDADE DOS RES√çDUOS\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Q-Q Plot\n",
    "qqplot(residuos2, line='45', ax=axes[0], markersize=2, alpha=0.3)\n",
    "axes[0].set_title('Q-Q Plot dos Res√≠duos\\n(Modelo 2: dti)', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Histograma\n",
    "sns.histplot(residuos2, kde=True, bins=50, color='darkorange', ax=axes[1])\n",
    "axes[1].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1].set_title('Distribui√ß√£o dos Res√≠duos\\n(Modelo 2: dti)', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Res√≠duo')\n",
    "axes[1].set_ylabel('Frequ√™ncia')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Shapiro-Wilk\n",
    "amostra_residuos2 = np.random.choice(residuos2, size=min(5000, len(residuos2)), replace=False)\n",
    "stat_sw2, p_sw2 = shapiro(amostra_residuos2)\n",
    "print(f\"üìä Teste de Shapiro-Wilk (Modelo 2):\")\n",
    "print(f\"   Estat√≠stica W = {stat_sw2:.6f}\")\n",
    "print(f\"   p-value       = {p_sw2:.6e}\")\n",
    "if p_sw2 > 0.05:\n",
    "    print(\"   ‚úÖ N√£o rejeitamos H‚ÇÄ: res√≠duos normais.\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  H‚ÇÄ rejeitada ‚Äî desvio detectado (esperado com n grande).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3e2817",
   "metadata": {},
   "source": [
    "### 23 ¬∑ Modelo 2: Valida√ß√£o das Premissas ‚Äî Independ√™ncia (Durbin-Watson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2b1fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 23. MODELO 2 ‚Äî VALIDA√á√ÉO: INDEPEND√äNCIA (DURBIN-WATSON)\n",
    "# =============================================================================\n",
    "\n",
    "dw2 = durbin_watson(modelo2_ols.resid)\n",
    "print(f\"üìä Estat√≠stica de Durbin-Watson (Modelo 2): {dw2:.4f}\")\n",
    "if 1.5 < dw2 < 2.5:\n",
    "    print(\"   ‚úÖ Sem autocorrela√ß√£o significativa.\")\n",
    "elif dw2 <= 1.5:\n",
    "    print(\"   ‚ö†Ô∏è  Poss√≠vel autocorrela√ß√£o positiva.\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Poss√≠vel autocorrela√ß√£o negativa.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444a240d",
   "metadata": {},
   "source": [
    "---\n",
    "# üî∑ MODELO 3: `fico_score` ‚Üí `int_rate`\n",
    "## Score FICO como Preditor da Taxa de Juros\n",
    "\n",
    "**Hip√≥tese de neg√≥cio:** O FICO score √© o **principal indicador de risco de cr√©dito** nos EUA. Quanto **maior o score**, menor o risco ‚Üí o banco cobra **juros menores**. Esperamos Œ≤‚ÇÅ **fortemente negativo**.\n",
    "\n",
    "Este deve ser o modelo com **melhor R¬≤** dentre os tr√™s, pois o FICO score √© explicitamente usado pelo Lending Club para definir os grades (A‚ÄìG) que determinam as faixas de taxa de juros.\n",
    "\n",
    "**Equa√ß√£o:**\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1 \\cdot \\text{fico\\_score}$$\n",
    "\n",
    "---\n",
    "\n",
    "### 24 ¬∑ Modelo 3: Regress√£o OLS (statsmodels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3e8d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 24. MODELO 3 ‚Äî fico_score ‚Üí int_rate (STATSMODELS OLS)\n",
    "# =============================================================================\n",
    "\n",
    "X3_train_sm = sm.add_constant(X3_train)\n",
    "X3_test_sm  = sm.add_constant(X3_test)\n",
    "\n",
    "modelo3_ols = sm.OLS(y3_train, X3_train_sm).fit()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODELO 3: fico_score ‚Üí int_rate (OLS Summary)\")\n",
    "print(\"=\" * 70)\n",
    "print(modelo3_ols.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f87e1f",
   "metadata": {},
   "source": [
    "### 25 ¬∑ Modelo 3: Previs√µes e M√©tricas de Avalia√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24921d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 25. MODELO 3 ‚Äî fico_score ‚Üí int_rate (PREVIS√ïES E M√âTRICAS)\n",
    "# =============================================================================\n",
    "\n",
    "# Coeficientes do OLS\n",
    "beta0_m3 = modelo3_ols.params[0]\n",
    "beta1_m3 = modelo3_ols.params[1]\n",
    "\n",
    "print(\"üìê Coeficientes (OLS statsmodels):\")\n",
    "print(f\"   Œ≤‚ÇÄ (intercepto):  {beta0_m3:.6f}\")\n",
    "print(f\"   Œ≤‚ÇÅ (fico_score):  {beta1_m3:.6f}\")\n",
    "\n",
    "# Previs√µes\n",
    "y3_pred = modelo3_ols.predict(X3_test_sm)\n",
    "\n",
    "# M√©tricas\n",
    "r2_m3   = r2_score_manual(y3_test, y3_pred)\n",
    "rmse_m3 = rmse_manual(y3_test, y3_pred)\n",
    "mae_m3  = mae_manual(y3_test, y3_pred)\n",
    "\n",
    "print(f\"\\nüìä M√©tricas no Conjunto de Teste:\")\n",
    "print(f\"   R¬≤:   {r2_m3:.6f}\")\n",
    "print(f\"   RMSE: {rmse_m3:.4f}%\")\n",
    "print(f\"   MAE:  {mae_m3:.4f}%\")\n",
    "\n",
    "# Scatter Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.scatter(X3_test, y3_test, alpha=0.15, s=8, color='seagreen', label='Dados reais (teste)')\n",
    "sorted_idx = X3_test.flatten().argsort()\n",
    "ax.plot(X3_test.flatten()[sorted_idx], y3_pred[sorted_idx],\n",
    "        color='darkred', linewidth=2, label='Reta de Regress√£o')\n",
    "ax.set_title('Modelo 3: fico_score ‚Üí int_rate (OLS)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Score FICO')\n",
    "ax.set_ylabel('Taxa de Juros (%)')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8808eaed",
   "metadata": {},
   "source": [
    "### 26 ¬∑ Modelo 3: Rede Neural Simples (TensorFlow/Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff022d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 26. MODELO 3 ‚Äî fico_score ‚Üí int_rate (REDE NEURAL - TENSORFLOW/KERAS)\n",
    "# =============================================================================\n",
    "\n",
    "# Padronizar fico_score (z-score manual)\n",
    "X3_train_scaled, X3_test_scaled, mean3, std3 = standard_scale(X3_train, X3_test)\n",
    "\n",
    "# Construir rede neural\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "modelo3_nn = Sequential([\n",
    "    Dense(1, input_shape=(1,), activation='linear')\n",
    "])\n",
    "modelo3_nn.compile(optimizer=Adam(learning_rate=0.01), loss='mse', metrics=['mae'])\n",
    "\n",
    "# Treinar\n",
    "print(\"‚è≥ Treinando Rede Neural ‚Äî Modelo 3 (fico_score ‚Üí int_rate)...\")\n",
    "history3 = modelo3_nn.fit(\n",
    "    X3_train_scaled, y3_train,\n",
    "    epochs=100, batch_size=32,\n",
    "    validation_split=0.1, verbose=0\n",
    ")\n",
    "print(\"‚úÖ Treino conclu√≠do!\")\n",
    "\n",
    "# Curva de Loss\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(history3.history['loss'], label='Loss (Treino)', linewidth=2)\n",
    "ax.plot(history3.history['val_loss'], label='Loss (Valida√ß√£o)', linewidth=2, linestyle='--')\n",
    "ax.set_title('Modelo 3 (NN): Curva de Loss por √âpoca', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('√âpoca')\n",
    "ax.set_ylabel('MSE')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Desnormalizar pesos\n",
    "W3_scaled, b3_scaled = modelo3_nn.get_weights()\n",
    "W3_real = W3_scaled[0][0] / std3[0]\n",
    "b3_real = b3_scaled[0] - W3_scaled[0][0] * mean3[0] / std3[0]\n",
    "print(f\"\\nüìê Pesos da Rede Neural (desnormalizados):\")\n",
    "print(f\"   W (‚âà Œ≤‚ÇÅ): {W3_real:.6f}\")\n",
    "print(f\"   b (‚âà Œ≤‚ÇÄ): {b3_real:.6f}\")\n",
    "print(f\"   Compara√ß√£o OLS ‚Üí Œ≤‚ÇÅ: {beta1_m3:.6f}, Œ≤‚ÇÄ: {beta0_m3:.6f}\")\n",
    "\n",
    "# M√©tricas\n",
    "y3_pred_nn = modelo3_nn.predict(X3_test_scaled, verbose=0).flatten()\n",
    "r2_m3_nn   = r2_score_manual(y3_test, y3_pred_nn)\n",
    "rmse_m3_nn = rmse_manual(y3_test, y3_pred_nn)\n",
    "mae_m3_nn  = mae_manual(y3_test, y3_pred_nn)\n",
    "print(f\"\\nüìä M√©tricas (Rede Neural):\")\n",
    "print(f\"   R¬≤:   {r2_m3_nn:.6f}\")\n",
    "print(f\"   RMSE: {rmse_m3_nn:.4f}%\")\n",
    "print(f\"   MAE:  {mae_m3_nn:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2c189a",
   "metadata": {},
   "source": [
    "### 27 ¬∑ Modelo 3: Valida√ß√£o das Premissas ‚Äî Linearidade e Homocedasticidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b5edeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 27. MODELO 3 ‚Äî VALIDA√á√ÉO: LINEARIDADE E HOMOCEDASTICIDADE\n",
    "# =============================================================================\n",
    "\n",
    "residuos3 = y3_test - y3_pred\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Res√≠duos vs ≈∑ ‚Üí Homocedasticidade\n",
    "axes[0].scatter(y3_pred, residuos3, alpha=0.15, s=8, color='seagreen')\n",
    "axes[0].axhline(y=0, color='red', linewidth=2, linestyle='--')\n",
    "axes[0].set_title('Res√≠duos vs Valores Previstos (≈∑)\\nHomocedasticidade', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('Valores Previstos (≈∑)')\n",
    "axes[0].set_ylabel('Res√≠duos')\n",
    "\n",
    "# Res√≠duos vs X ‚Üí Linearidade\n",
    "axes[1].scatter(X3_test, residuos3, alpha=0.15, s=8, color='coral')\n",
    "axes[1].axhline(y=0, color='red', linewidth=2, linestyle='--')\n",
    "axes[1].set_title('Res√≠duos vs fico_score\\nLinearidade', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Score FICO')\n",
    "axes[1].set_ylabel('Res√≠duos')\n",
    "\n",
    "plt.suptitle('Modelo 3 ‚Äî An√°lise de Res√≠duos', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5ddec8",
   "metadata": {},
   "source": [
    "### 28 ¬∑ Modelo 3: Valida√ß√£o das Premissas ‚Äî Normalidade dos Res√≠duos (Q-Q Plot + Shapiro-Wilk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f39c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 28. MODELO 3 ‚Äî VALIDA√á√ÉO: NORMALIDADE DOS RES√çDUOS\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Q-Q Plot\n",
    "qqplot(residuos3, line='45', ax=axes[0], markersize=2, alpha=0.3)\n",
    "axes[0].set_title('Q-Q Plot dos Res√≠duos\\n(Modelo 3: fico_score)', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Histograma\n",
    "sns.histplot(residuos3, kde=True, bins=50, color='seagreen', ax=axes[1])\n",
    "axes[1].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1].set_title('Distribui√ß√£o dos Res√≠duos\\n(Modelo 3: fico_score)', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Res√≠duo')\n",
    "axes[1].set_ylabel('Frequ√™ncia')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Shapiro-Wilk\n",
    "amostra_residuos3 = np.random.choice(residuos3, size=min(5000, len(residuos3)), replace=False)\n",
    "stat_sw3, p_sw3 = shapiro(amostra_residuos3)\n",
    "print(f\"üìä Teste de Shapiro-Wilk (Modelo 3):\")\n",
    "print(f\"   Estat√≠stica W = {stat_sw3:.6f}\")\n",
    "print(f\"   p-value       = {p_sw3:.6e}\")\n",
    "if p_sw3 > 0.05:\n",
    "    print(\"   ‚úÖ N√£o rejeitamos H‚ÇÄ: res√≠duos normais.\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  H‚ÇÄ rejeitada ‚Äî desvio detectado (esperado com n grande).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b7e590",
   "metadata": {},
   "source": [
    "### 29 ¬∑ Modelo 3: Valida√ß√£o das Premissas ‚Äî Independ√™ncia (Durbin-Watson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50847807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 29. MODELO 3 ‚Äî VALIDA√á√ÉO: INDEPEND√äNCIA (DURBIN-WATSON)\n",
    "# =============================================================================\n",
    "\n",
    "dw3 = durbin_watson(modelo3_ols.resid)\n",
    "print(f\"üìä Estat√≠stica de Durbin-Watson (Modelo 3): {dw3:.4f}\")\n",
    "if 1.5 < dw3 < 2.5:\n",
    "    print(\"   ‚úÖ Sem autocorrela√ß√£o significativa.\")\n",
    "elif dw3 <= 1.5:\n",
    "    print(\"   ‚ö†Ô∏è  Poss√≠vel autocorrela√ß√£o positiva.\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Poss√≠vel autocorrela√ß√£o negativa.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9731b467",
   "metadata": {},
   "source": [
    "---\n",
    "# üìä COMPARA√á√ÉO E CONCLUS√ïES\n",
    "\n",
    "## 30 ¬∑ Tabela Comparativa dos 3 Modelos\n",
    "\n",
    "Reunimos todas as m√©tricas lado a lado para comparar o desempenho de cada vari√°vel independente como preditora da taxa de juros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49d595c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 30. TABELA COMPARATIVA DOS 3 MODELOS\n",
    "# =============================================================================\n",
    "\n",
    "# Extrair p-values dos coeficientes (Œ≤‚ÇÅ) de cada modelo OLS\n",
    "p_m1 = modelo1_ols.pvalues[1]  # √çndice 1 = coeficiente de X (√≠ndice 0 = constante)\n",
    "p_m2 = modelo2_ols.pvalues[1]\n",
    "p_m3 = modelo3_ols.pvalues[1]\n",
    "\n",
    "# Criar tabela comparativa\n",
    "comparacao = pd.DataFrame({\n",
    "    'Modelo': ['Modelo 1', 'Modelo 2', 'Modelo 3'],\n",
    "    'Vari√°vel (X)': ['annual_inc', 'dti', 'fico_score'],\n",
    "    'R¬≤': [r2_m1, r2_m2, r2_m3],\n",
    "    'R¬≤ Ajustado': [modelo1_ols.rsquared_adj, modelo2_ols.rsquared_adj, modelo3_ols.rsquared_adj],\n",
    "    'RMSE (%)': [rmse_m1, rmse_m2, rmse_m3],\n",
    "    'MAE (%)': [mae_m1, mae_m2, mae_m3],\n",
    "    'Coeficiente (Œ≤‚ÇÅ)': [beta1_m1, beta1_m2, beta1_m3],\n",
    "    'p-value (Œ≤‚ÇÅ)': [p_m1, p_m2, p_m3],\n",
    "    'Durbin-Watson': [dw1, dw2, dw3]\n",
    "})\n",
    "\n",
    "# Formatar para melhor visualiza√ß√£o\n",
    "print(\"=\" * 90)\n",
    "print(\"üìä TABELA COMPARATIVA ‚Äî MODELOS DE REGRESS√ÉO LINEAR SIMPLES\")\n",
    "print(\"=\" * 90)\n",
    "comparacao_display = comparacao.copy()\n",
    "comparacao_display['R¬≤'] = comparacao_display['R¬≤'].apply(lambda x: f\"{x:.6f}\")\n",
    "comparacao_display['R¬≤ Ajustado'] = comparacao_display['R¬≤ Ajustado'].apply(lambda x: f\"{x:.6f}\")\n",
    "comparacao_display['RMSE (%)'] = comparacao_display['RMSE (%)'].apply(lambda x: f\"{x:.4f}\")\n",
    "comparacao_display['MAE (%)'] = comparacao_display['MAE (%)'].apply(lambda x: f\"{x:.4f}\")\n",
    "comparacao_display['Coeficiente (Œ≤‚ÇÅ)'] = comparacao_display['Coeficiente (Œ≤‚ÇÅ)'].apply(lambda x: f\"{x:.8f}\")\n",
    "comparacao_display['p-value (Œ≤‚ÇÅ)'] = comparacao_display['p-value (Œ≤‚ÇÅ)'].apply(lambda x: f\"{x:.2e}\")\n",
    "comparacao_display['Durbin-Watson'] = comparacao_display['Durbin-Watson'].apply(lambda x: f\"{x:.4f}\")\n",
    "comparacao_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2ab936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Gr√°ficos Comparativos ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "nomes = ['annual_inc\\n(Renda)', 'dti\\n(D√≠vida/Renda)', 'fico_score\\n(Score FICO)']\n",
    "cores = ['steelblue', 'darkorange', 'seagreen']\n",
    "\n",
    "# R¬≤\n",
    "r2_vals = [r2_m1, r2_m2, r2_m3]\n",
    "bars1 = axes[0].bar(nomes, r2_vals, color=cores, edgecolor='black', linewidth=0.5)\n",
    "axes[0].set_title('Compara√ß√£o de R¬≤ (Coef. Determina√ß√£o)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('R¬≤')\n",
    "for bar, val in zip(bars1, r2_vals):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "                 f'{val:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# RMSE\n",
    "rmse_vals = [rmse_m1, rmse_m2, rmse_m3]\n",
    "bars2 = axes[1].bar(nomes, rmse_vals, color=cores, edgecolor='black', linewidth=0.5)\n",
    "axes[1].set_title('Compara√ß√£o de RMSE (Erro M√©dio)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('RMSE (%)')\n",
    "for bar, val in zip(bars2, rmse_vals):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                 f'{val:.3f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.suptitle('Compara√ß√£o dos 3 Modelos de Regress√£o Linear Simples',\n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Ranking ---\n",
    "print(\"\\nüèÜ RANKING DOS MODELOS (por R¬≤, do melhor ao pior):\")\n",
    "ranking = comparacao.sort_values('R¬≤', ascending=False)\n",
    "for i, (_, row) in enumerate(ranking.iterrows(), 1):\n",
    "    print(f\"   {i}¬∫ lugar: {row['Modelo']} ({row['Vari√°vel (X)']}) ‚Äî R¬≤ = {row['R¬≤']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f42066a",
   "metadata": {},
   "source": [
    "---\n",
    "## 31 ¬∑ B√¥nus: Regress√£o M√∫ltipla e C√°lculo do VIF (Multicolinearidade)\n",
    "\n",
    "Combinamos as **3 vari√°veis** em um √∫nico modelo de **Regress√£o M√∫ltipla** para verificar se, juntas, elas explicam mais da vari√¢ncia de `int_rate`.\n",
    "\n",
    "**Equa√ß√£o:**\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1 \\cdot \\text{annual\\_inc} + \\beta_2 \\cdot \\text{dti} + \\beta_3 \\cdot \\text{fico\\_score}$$\n",
    "\n",
    "**VIF (Variance Inflation Factor)** mede a **multicolinearidade** entre as vari√°veis independentes:\n",
    "\n",
    "| VIF | Interpreta√ß√£o |\n",
    "|---|---|\n",
    "| 1 | Sem correla√ß√£o entre as vari√°veis |\n",
    "| 1‚Äì5 | Correla√ß√£o moderada (aceit√°vel) |\n",
    "| 5‚Äì10 | Correla√ß√£o alta (preocupante) |\n",
    "| > 10 | Multicolinearidade severa (coeficientes inst√°veis) |\n",
    "\n",
    "> Se o VIF for alto, os coeficientes individuais podem ser inst√°veis (mudam muito com pequenas altera√ß√µes nos dados), mesmo que o modelo geral tenha bom R¬≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70424f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 31. B√îNUS: REGRESS√ÉO M√öLTIPLA + VIF (MULTICOLINEARIDADE)\n",
    "# =============================================================================\n",
    "\n",
    "# --- Preparar dados com todas as 3 vari√°veis ---\n",
    "X_multi = df[['annual_inc', 'dti', 'fico_score']].values\n",
    "y_multi = df['int_rate'].values\n",
    "\n",
    "X_multi_train, X_multi_test, y_multi_train, y_multi_test = train_test_split_manual(\n",
    "    X_multi, y_multi, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# --- OLS com statsmodels ---\n",
    "X_multi_train_sm = sm.add_constant(X_multi_train)\n",
    "X_multi_test_sm  = sm.add_constant(X_multi_test)\n",
    "\n",
    "modelo_multi = sm.OLS(y_multi_train, X_multi_train_sm).fit()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"B√îNUS: REGRESS√ÉO M√öLTIPLA (annual_inc + dti + fico_score ‚Üí int_rate)\")\n",
    "print(\"=\" * 70)\n",
    "print(modelo_multi.summary())\n",
    "\n",
    "# --- Calcular VIF ---\n",
    "# O VIF √© calculado para cada vari√°vel independente.\n",
    "# VIF_j = 1 / (1 - R¬≤_j), onde R¬≤_j √© o R¬≤ da regress√£o de X_j contra as outras X's.\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"üìä VIF ‚Äî Variance Inflation Factor\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Vari√°vel'] = ['annual_inc', 'dti', 'fico_score']\n",
    "vif_data['VIF'] = [\n",
    "    variance_inflation_factor(X_multi_train_sm, i) \n",
    "    for i in range(1, X_multi_train_sm.shape[1])  # Come√ßa em 1 para pular a constante\n",
    "]\n",
    "\n",
    "for _, row in vif_data.iterrows():\n",
    "    status = '‚úÖ OK' if row['VIF'] < 5 else '‚ö†Ô∏è  Alta' if row['VIF'] < 10 else 'üö® Severa'\n",
    "    print(f\"   {row['Vari√°vel']:15s} ‚Üí VIF = {row['VIF']:.4f}  ({status})\")\n",
    "\n",
    "# --- M√©tricas do modelo m√∫ltiplo (usando OLS) ---\n",
    "y_multi_pred = modelo_multi.predict(X_multi_test_sm)\n",
    "\n",
    "r2_multi   = r2_score_manual(y_multi_test, y_multi_pred)\n",
    "rmse_multi = rmse_manual(y_multi_test, y_multi_pred)\n",
    "mae_multi  = mae_manual(y_multi_test, y_multi_pred)\n",
    "\n",
    "print(f\"\\nüìä M√©tricas da Regress√£o M√∫ltipla (Teste):\")\n",
    "print(f\"   R¬≤:   {r2_multi:.6f}\")\n",
    "print(f\"   RMSE: {rmse_multi:.4f}%\")\n",
    "print(f\"   MAE:  {mae_multi:.4f}%\")\n",
    "print(f\"\\n   Compara√ß√£o com melhor modelo simples (fico_score): R¬≤ = {r2_m3:.6f}\")\n",
    "print(f\"   Ganho de R¬≤: +{(r2_multi - r2_m3):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a0627b",
   "metadata": {},
   "source": [
    "---\n",
    "# üéì Como Explicar Isso Para o Seu Professor ‚Äî Guia de Defesa\n",
    "\n",
    "## Resumo Executivo das M√©tricas\n",
    "\n",
    "### 1. R¬≤ (Coeficiente de Determina√ß√£o)\n",
    "> *\"O R¬≤ nos diz qual percentual da vari√¢ncia da taxa de juros √© explicado pela vari√°vel X.\"*\n",
    "\n",
    "- Se o R¬≤ do Modelo 3 (fico_score) for, por exemplo, 0.42, voc√™ diz: **\"O score FICO, sozinho, explica 42% da varia√ß√£o na taxa de juros.\"**\n",
    "- Em Regress√£o Linear **Simples**, R¬≤ entre 0.10 e 0.50 √© **informativo e esperado**, pois a taxa de juros depende de m√∫ltiplos fatores (risco, mercado, pol√≠tica da empresa).\n",
    "- R¬≤ baixo **n√£o significa que o modelo √© in√∫til** ‚Äî significa que a vari√°vel √© **um** dos fatores, n√£o o √∫nico.\n",
    "\n",
    "### 2. RMSE (Root Mean Squared Error)\n",
    "> *\"O RMSE nos diz, em m√©dia, quantos pontos percentuais o modelo erra na previs√£o da taxa.\"*\n",
    "\n",
    "- Se o RMSE for 3.5%, significa que o modelo **tipicamente erra em ¬±3.5 pontos percentuais**. Para uma taxa real de 15%, o modelo pode prever entre 11.5% e 18.5%.\n",
    "- O RMSE √© em **unidades da vari√°vel-alvo** (%), o que facilita a interpreta√ß√£o pr√°tica.\n",
    "- O RMSE **penaliza mais erros grandes** (por causa da raiz do quadrado), ent√£o √© mais conservador que o MAE.\n",
    "\n",
    "### 3. MAE (Mean Absolute Error)\n",
    "> *\"O MAE √© o erro m√©dio absoluto ‚Äî quanto o modelo erra, em m√©dia, sem penalizar erros grandes.\"*\n",
    "\n",
    "- Se o MAE for 2.8%, o erro t√≠pico √© de 2.8 pontos percentuais.\n",
    "- Se RMSE >> MAE, significa que existem alguns erros muito grandes (outliers nos res√≠duos).\n",
    "\n",
    "### 4. p-value dos Coeficientes\n",
    "> *\"O p-value nos diz se a rela√ß√£o entre X e Y √© estatisticamente significativa ou se pode ser obra do acaso.\"*\n",
    "\n",
    "- **p < 0.05**: H√° evid√™ncia estat√≠stica de que a vari√°vel X influencia `int_rate`. O coeficiente √© significativo.\n",
    "- **p ‚â• 0.05**: N√£o podemos afirmar com confian√ßa que X influencia Y. O coeficiente pode ser zero na popula√ß√£o.\n",
    "- Na pr√°tica, se os p-values de todos os 3 modelos forem < 0.001 (muito pequenos), **todas as vari√°veis escolhidas s√£o significativas**.\n",
    "\n",
    "### 5. Coeficiente Œ≤‚ÇÅ ‚Äî Interpreta√ß√£o Pr√°tica\n",
    "> *\"O Œ≤‚ÇÅ nos diz quanto a taxa de juros muda para cada unidade a mais na vari√°vel X.\"*\n",
    "\n",
    "Exemplos de como falar na apresenta√ß√£o:\n",
    "- **annual_inc:** *\"Para cada d√≥lar a mais de renda anual, a taxa de juros diminui em Œ≤‚ÇÅ pontos percentuais. Como Œ≤‚ÇÅ √© muito pequeno (ex: -0.00001), faz mais sentido dizer: para cada $10.000 a mais, a taxa cai em ~0.1 pp.\"*\n",
    "- **dti:** *\"Para cada ponto percentual a mais no DTI, a taxa de juros aumenta em Œ≤‚ÇÅ pp.\"*\n",
    "- **fico_score:** *\"Para cada ponto a mais no FICO score, a taxa de juros diminui em Œ≤‚ÇÅ pp.\"*\n",
    "\n",
    "### 6. An√°lise de Res√≠duos ‚Äî Por que fizemos?\n",
    "> *\"As premissas do OLS (M√≠nimos Quadrados Ordin√°rios) precisam ser satisfeitas para que os intervalos de confian√ßa e p-values sejam v√°lidos.\"*\n",
    "\n",
    "| Premissa | O que verificamos | O que acontece se falhar |\n",
    "|---|---|---|\n",
    "| **Linearidade** | Scatter de res√≠duos vs X sem padr√£o curvil√≠neo | O modelo linear √© inadequado; precisar√≠amos de transforma√ß√µes ou modelos n√£o-lineares |\n",
    "| **Homocedasticidade** | Scatter de res√≠duos vs ≈∑ sem formato de funil | Erros-padr√£o ficam enviesados ‚Üí p-values n√£o confi√°veis |\n",
    "| **Normalidade dos Res√≠duos** | Q-Q Plot + Shapiro-Wilk | Intervalos de confian√ßa e testes de hip√≥tese ficam imprecisos |\n",
    "| **Independ√™ncia** | Durbin-Watson ‚âà 2 | Erros-padr√£o subestimados ‚Üí testes de signific√¢ncia otimistas |\n",
    "\n",
    "### 7. Por que a Rede Neural deu resultado (quase) igual?\n",
    "> *\"Um neur√¥nio com ativa√ß√£o linear computa ≈∑ = Wx + b, que √© algebricamente id√™ntico √† equa√ß√£o da regress√£o linear ≈∑ = Œ≤‚ÇÅx + Œ≤‚ÇÄ. A diferen√ßa √© apenas o m√©todo de otimiza√ß√£o: OLS encontra a solu√ß√£o anal√≠tica exata; a rede neural chega ao mesmo lugar via gradient descent iterativo.\"*\n",
    "\n",
    "Isso demonstra que:\n",
    "- Regress√£o linear √© um **caso particular** de rede neural.\n",
    "- Redes neurais ganham poder com **camadas adicionais** e **ativa√ß√µes n√£o-lineares** (ReLU, Sigmoid), que n√£o usamos aqui propositalmente.\n",
    "\n",
    "### 8. Limita√ß√µes e Honestidade Acad√™mica\n",
    "> *\"R¬≤ baixo em regress√£o simples √© esperado e aceit√°vel. O objetivo n√£o √© construir o melhor modelo preditivo poss√≠vel, mas sim demonstrar que cada vari√°vel, isoladamente, possui rela√ß√£o estatisticamente significativa com a taxa de juros, validando as premissas do OLS e interpretando os resultados com rigor.\"*\n",
    "\n",
    "---\n",
    "\n",
    "**Frase de encerramento sugerida:**\n",
    "\n",
    "> *\"Este trabalho demonstrou, com rigor estat√≠stico, que renda anual, DTI e score FICO possuem rela√ß√£o significativa com a taxa de juros (todos com p < 0.05), sendo o FICO score o preditor mais forte. As premissas da regress√£o foram verificadas e discutidas, e a equival√™ncia entre regress√£o linear e rede neural de um neur√¥nio foi demonstrada matematicamente e empiricamente.\"*\n",
    "\n",
    "---\n",
    "\n",
    "<p align=\"center\">\n",
    "  <strong>üìö Projeto desenvolvido por Caio Thomas Silva Bandeira ‚Äî CEUB 2026</strong>\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
